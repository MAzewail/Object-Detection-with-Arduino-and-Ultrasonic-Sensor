# -*- coding: utf-8 -*-
"""Copy of Radar.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lWHMs9xZzhq5RrBt2lJu6OpsjF7DGIr0
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import SGDClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import KFold

"""# Binary Classification

40 sampels and 2 classes
"""

data = np.load('/content/Radar_Data_2_50.npy')

data.shape

df = pd.DataFrame(data)

df

df.describe()

for i in range(df.shape[0]//2):
  df.iloc[i,:].plot()
plt.figure()
for i in range(df.shape[0]//2,df.shape[0]):
  df.iloc[i,:].plot()

# print(df.iloc[0,:])
# print('====================')
# print(df.iloc[0,::-1])

# for i in range(1,df.shape[0],2):
#   df.iloc[i,:] = df.iloc[i,::-1].copy()
# df1 = df.copy()
# df.iloc[df.shape[0]//2:df.shape[0],:]=0
# for i in range(df.shape[0]//2,df.shape[0]):
#     # print(i)
#     # print(df.iloc[i,:])
#     # print(df.iloc[i,::-1])
#   df.iloc[i,:] = df1.iloc[i,::-1].copy()
# # for i in range(50,df.shape[0]):
# #   df.iloc[i,:] =0
# # for i in range(50,df.shape[0]):
# #   for ii in range(180):
# #     # print(i)
# #     # print(df.iloc[i,:])
# #     # print(df.iloc[i,::-1])
# #     df.iloc[i,ii] = df1.iloc[i,179-ii].copy()

df['class'] = 0
df.iloc[(data.shape[0]//2):,-1] = 1
# df['class'][20:30] = 2
df['class']

df

print(df.iloc[0,:])
print('===================')
print(df.iloc[75,:])

for i in range(0,50):#df.shape[0]):
  df.iloc[i,:].plot()
plt.figure()
for i in range(50,100):#df.shape[0]):
  df.iloc[i,:].plot()



# df[df.iloc[:,:-1] == 0] = 50

# df

from sklearn.model_selection import train_test_split
from sklearn.linear_model import SGDClassifier
from  sklearn.preprocessing import StandardScaler
from sklearn.model_selection import KFold

# x_train , y_train = df.iloc[:,:-1], df.iloc[:,-1]
# kf = KFold(n_splits=100)
# for x_train , x_test in kf.split(df.iloc[:,:-1]):
#   print(x_train , x_test)

x_train , y_train = df.iloc[:,:-1], df.iloc[:,-1]
x_train, x_test, y_train, y_test = train_test_split(x_train,y_train,test_size=0.3,shuffle=True)

"""# Classic ML"""

scaler = StandardScaler()
scaler.fit(x_train)
x_train = scaler.transform(x_train)
x_test = scaler.transform(x_test)

clf = SGDClassifier()
clf.fit(x_train,y_train)

x_test.shape

for i in range(y_test.shape[0]):
  print(clf.predict(x_test[i,:].reshape(1,-1)), y_test.iloc[i])

preds = clf.predict(x_test)

acc = []
for i in range(len(y_test)):
  if preds[i] != y_test.iloc[i]:
    acc.append(0)
  else:
    acc.append(1)
np.sum(acc)/len(acc)

"""# SVM"""

from sklearn.svm import SVC

svm_clf = SVC()
svm_clf.fit(x_train,y_train)

for i in range(y_test.shape[0]):
  print(svm_clf.predict(x_test[i,:].reshape(1,-1)), y_test.iloc[i])
  #print(svm_clf.predict(np.array(x_test.iloc[i,:]).reshape(1,-1)), y_test.iloc[i])

preds = svm_clf.predict(x_test)

acc = []
for i in range(len(y_test)):
  if preds[i] != y_test.iloc[i]:
    acc.append(0)
  else:
    acc.append(1)
np.sum(acc)/len(acc)

"""# Decsion tree"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

ens_class = DecisionTreeClassifier(max_depth=10, min_samples_leaf=5).fit(x_train,y_train)
ens_class = RandomForestClassifier(n_estimators=10, max_depth=10, min_samples_leaf=5).fit(x_train,y_train)

for i in range(y_test.shape[0]):
  print(ens_class.predict(x_test[i,:].reshape(1,-1)), y_test.iloc[i])
  #print(svm_clf.predict(np.array(x_test.iloc[i,:]).reshape(1,-1)), y_test.iloc[i])

preds = ens_class.predict(x_test)

acc = []
for i in range(len(y_test)):
  x = 0 if preds[i] < 0.5 else 1
  if x != y_test.iloc[i]:
    acc.append(0)
  else:
    acc.append(1)
np.sum(acc)/len(acc)

"""# ANN"""

from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.optimizers import Adam,RMSprop,SGD

model = Sequential()
model.add(Dense(units=256,activation='relu',input_shape=(180,)))
model.add(Dropout(0.1))
model.add(Dense(128,activation='relu'))
model.add(Dropout(0.1))
model.add(Dense(64,activation='relu'))
model.add(Dense(1,activation='sigmoid'))
# model.compile(optimizer='adam',loss='catigorical_cross_entropy',metrics='accuracy')
model.summary()

model.compile(optimizer=Adam(),loss='binary_crossentropy',metrics='accuracy')

from keras.callbacks import ModelCheckpoint
check_point = ModelCheckpoint('trained_model1.h5',monitor='val_loss',save_best_only=True,verbose=1)

history = model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=500,batch_size=256,callbacks=[check_point])

for i in ['loss','val_loss']:
  plt.plot(range(len(history.history[f'{i}'])),history.history[f'{i}'],label=f'{i}')
plt.title('Treaining and Validation loss for ANN model')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.figure()
for i in ['accuracy','val_accuracy']:
  plt.plot(range(len(history.history[f'{i}'])),history.history[f'{i}'],label=f'{i}')
plt.title('Treaining and Validation Accuracy for ANN model')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

from keras.models import load_model
model1 = load_model('/content/trained_model1.h5')

model.evaluate(x_test,y_test)

model1.evaluate(x_test,y_test)

x_test.shape

for i in range(y_test.shape[0]):
  if model.predict(np.array(x_test.iloc[i,:]).reshape(1,-1),verbose=0) < 0.5:
    print(0, y_test.iloc[i])
  else:
    print(1, y_test.iloc[i])

for i in range(y_test.shape[0]):
  if model1.predict(np.array(x_test.iloc[i,:]).reshape(1,-1),verbose=0) < 0.5:
    print(0, y_test.iloc[i])
  else:
    print(1, y_test.iloc[i])



"""# CNN"""

from keras.layers import Conv1D, MaxPool1D, Dense, Dropout, Flatten, GlobalAvgPool1D
from keras.models import Sequential
import keras
import tensorflow as tf

keras.backend.clear_session()
tf.random.set_seed(42)
np.random.seed(42)

cnn_model = Sequential()
cnn_model.add(Conv1D(64,3,activation='relu',padding='same',input_shape=(180,1)))
cnn_model.add(MaxPool1D(2))
cnn_model.add(Conv1D(128,3,activation='relu',padding='same'))
cnn_model.add(MaxPool1D(2))
cnn_model.add(GlobalAvgPool1D())#Flatten())
cnn_model.add(Dense(units=128,activation='relu'))
cnn_model.add(Dropout(0.1))
cnn_model.add(Dense(units=1,activation='sigmoid'))
cnn_model.summary()

cnn_model.compile(optimizer=Adam(),loss='binary_crossentropy',metrics='accuracy')

from keras.callbacks import ModelCheckpoint
check_point = ModelCheckpoint('trained_cnnmodel.h5',monitor='val_loss',save_best_only=True,verbose=1)

# lr_schedule = keras.callbacks.LearningRateScheduler(
#     lambda epoch: 1e-6 * 10**(epoch / 10))
history_cnn = cnn_model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=1500,batch_size=256,callbacks=[check_point])#,lr_schedule])

for i in ['loss','val_loss']:
  plt.plot(range(len(history_cnn.history[f'{i}'])),history_cnn.history[f'{i}'],label=f'{i}')
plt.title('Treaining and Validation loss for CNN model')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.figure()
for i in ['accuracy','val_accuracy']:
  plt.plot(range(len(history_cnn.history[f'{i}'])),history_cnn.history[f'{i}'],label=f'{i}')
plt.title('Treaining and Validation Accuracy for CNN model')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

from keras.models import load_model
model1 = load_model('/content/trained_cnnmodel.h5')

cnn_model.evaluate(x_test,y_test)

model1.evaluate(x_test,y_test)

x_test.shape

for i in range(y_test.shape[0]):
  if cnn_model.predict(np.array(x_test.iloc[i,:]).reshape(1,-1),verbose=0) < 0.5:
    print(0, y_test.iloc[i])
  else:
    print(1, y_test.iloc[i])

for i in range(y_test.shape[0]):
  if model1.predict(np.array(x_test.iloc[i,:]).reshape(1,-1),verbose=0) < 0.5:
    print(0, y_test.iloc[i])
  else:
    print(1, y_test.iloc[i])

